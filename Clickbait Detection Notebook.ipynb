{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJIAQTsAzkZ0"
      },
      "source": [
        "# Overview\n",
        "This notebook contains the code for clickbait detecting Bert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjihAVr90bDo"
      },
      "outputs": [],
      "source": [
        "#This code block has just standard setup code for running in Python\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "\n",
        "# Fix the random seed for reproducability\n",
        "torch.random.manual_seed(8942764)\n",
        "torch.cuda.manual_seed(8942764)\n",
        "np.random.seed(8942764)\n",
        "\n",
        "# Please set your device by uncommenting the right version below\n",
        "\n",
        "# On colab or on a machine with access to an Nvidia GPU  use the following setting\n",
        "device = 'cuda:0'\n",
        "\n",
        "# if you have an Apple Silicon machine with a GPU, use the following setting\n",
        "# this should about 3-4 times faster that running it on a plain CPU\n",
        "# device = 'mps'\n",
        "\n",
        "# If you will use a cpu, this is the setting\n",
        "# device='cpu'\n",
        "\n",
        "# note that in handin.py these next two steps would need to be removed\n",
        "# if you are going run this on you personal machine these would need to be done\n",
        "# in the shell/terminal to update your python libraries\n",
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duHZ1XZMoYkZ"
      },
      "outputs": [],
      "source": [
        "# load the data set from the huggingface repositories. Please make sure the dataset is modified to be in a similar format.\n",
        "\n",
        "dataset = load_dataset(\"christinacdl/clickbait_notclickbait_dataset\")\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaqtJZZFmDMf"
      },
      "outputs": [],
      "source": [
        "# initialize pretrained BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uv_urtjmQH2"
      },
      "outputs": [],
      "source": [
        "# if you want you can look at some sample  data items\n",
        "print(dataset[\"train\"][8])\n",
        "print(dataset[\"validation\"][6])\n",
        "print(dataset[\"test\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utsg41nOizGz"
      },
      "outputs": [],
      "source": [
        "# This dataset has 3 splits, train, validation and test, and each has a  text  and label.\n",
        "\n",
        "# Data from the dataset can generally be accessed like a Python dict.\n",
        "\n",
        "\n",
        "# Print the original sentence.\n",
        "print('Original: ', dataset['train'][8]['text'])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(dataset['train'][8]['text']))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(dataset['train'][8]['text'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83BUDj7AKmEo"
      },
      "outputs": [],
      "source": [
        "#code for tokenizing training data\n",
        "def tokenize(batch):\n",
        "  '''\n",
        "  Transform the text under the 'sentence' key to\n",
        "    batch has the following structure:\n",
        "    [\n",
        "      {\n",
        "        k1: v1,\n",
        "        k2: v2,\n",
        "        ...\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "  '''\n",
        "  sentences = [x['text'] for x in batch]\n",
        "  labels = torch.LongTensor([x['label'] for x in batch])\n",
        "  new_batch = dict(tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\"))\n",
        "  new_batch['label'] = labels\n",
        "  return new_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQcF7uNCZ7qp"
      },
      "outputs": [],
      "source": [
        "# This code evaluates a trained model on a dataset. It also uses  train() to train model\n",
        "# You probably should not be making any changes to this code.\n",
        "# During training, it will be printing some progress messages\n",
        "from tqdm import tqdm\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataset, batch_size, device, collate_fn=None):\n",
        "  model = model.eval().to(device)\n",
        "  dataloader = DataLoader(dataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "  lossfn = nn.NLLLoss()\n",
        "\n",
        "  loss_history = []\n",
        "  acc_history = []\n",
        "  for i, batch in enumerate(dataloader):\n",
        "      batch = {k:v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}\n",
        "      y = batch.pop('label')\n",
        "\n",
        "      logits = model(**batch)\n",
        "      loss = lossfn(logits, y)\n",
        "\n",
        "      pred = logits.argmax(1)\n",
        "      acc = (pred == y).float().mean()\n",
        "      loss_history.append(loss.item())\n",
        "      acc_history.append(acc.item())\n",
        "  return np.mean(loss_history), np.mean(acc_history)\n",
        "\n",
        "def train(model,\n",
        "          train_dataset,\n",
        "          val_dataset,\n",
        "          num_epochs,\n",
        "          batch_size,\n",
        "          optimizer_cls,\n",
        "          lr,\n",
        "          weight_decay,\n",
        "          device,\n",
        "          collate_fn=None):\n",
        "  model = model.train().to(device)\n",
        "  dataloader = DataLoader(train_dataset, batch_size, shuffle=True,\n",
        "                          collate_fn=collate_fn)\n",
        "\n",
        "  if optimizer_cls == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay=weight_decay)\n",
        "  elif optimizer_cls == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
        "\n",
        "  train_loss_history = []\n",
        "  train_acc_history = []\n",
        "  val_loss_history = []\n",
        "  val_acc_history = []\n",
        "\n",
        "  lossfn = nn.NLLLoss()\n",
        "  for e in tqdm(range(num_epochs)):\n",
        "    epoch_loss_history = []\n",
        "    epoch_acc_history = []\n",
        "    for i, batch in enumerate(dataloader):\n",
        "      batch = {k:v.to(device) for k,v in batch.items() if isinstance(v, torch.Tensor)}\n",
        "      y = batch.pop('label')\n",
        "\n",
        "      logits = model(**batch)\n",
        "      loss = lossfn(logits, y)\n",
        "\n",
        "      pred = logits.argmax(1)\n",
        "      acc = (pred == y).float().mean()\n",
        "\n",
        "      epoch_loss_history.append(loss.item())\n",
        "      epoch_acc_history.append(acc.item())\n",
        "\n",
        "      if (i % 100 == 0):\n",
        "        print(f'epoch: {e}\\t iter: {i}\\t train_loss: {np.mean(epoch_loss_history):.3e}\\t train_accuracy:{np.mean(epoch_acc_history):.3f}')\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    val_loss, val_acc = evaluate(model, val_dataset, batch_size, device, collate_fn=collate_fn)\n",
        "\n",
        "    train_loss_history.append(np.mean(epoch_loss_history))\n",
        "    train_acc_history.append(np.mean(epoch_acc_history))\n",
        "    val_loss_history.append(val_loss.item())\n",
        "    val_acc_history.append(val_acc.item())\n",
        "    print(f'epoch: {e}\\t train_loss: {train_loss_history[-1]:.3e}\\t train_accuracy:{train_acc_history[-1]:.3f}\\t val_loss: {val_loss_history[-1]:.3e}\\t val_accuracy:{val_acc_history[-1]:.3f}')\n",
        "\n",
        "  return model, (train_loss_history, train_acc_history, val_loss_history, val_acc_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb0wjC-zTRz0"
      },
      "outputs": [],
      "source": [
        "# This code defines the test classification class using BERT.\n",
        "# The classifier is defined on top of the final layer of BERT.\n",
        "\n",
        "class BertForTextClassification(nn.Module):\n",
        "  def __init__(self, bert_pretrained_config_name, num_classes, freeze_bert=False):\n",
        "    '''\n",
        "    BeRT with a classification MLP\n",
        "    args:\n",
        "    - bert_pretrained_config_name (str): model name from huggingface hub\n",
        "    - num_classes (int): number of classes in the classification task\n",
        "    - freeze_bert (bool): [default False] If true gradients are not computed for\n",
        "                          BeRT's parameters.\n",
        "    '''\n",
        "    super().__init__()\n",
        "    self.bert = BertModel.from_pretrained(bert_pretrained_config_name)\n",
        "    self.bert.requires_grad_(not freeze_bert)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(self.bert.config.hidden_size, 132),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(132, num_classes),\n",
        "        nn.LogSoftmax(dim=-1)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, **bert_kwargs):\n",
        "     output=self.bert(**bert_kwargs)\n",
        "     cls_embed = output.pooler_output\n",
        "     logits = self.classifier(cls_embed)\n",
        "     return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6NVHqYSYds-"
      },
      "outputs": [],
      "source": [
        "# This is where fine-tuning of the classifier happens.\n",
        "# It is set to batch size 32 for 5 epochs, feel free to fine-tune.\n",
        "\n",
        "torch.random.manual_seed(8942764)\n",
        "torch.cuda.manual_seed(8942764)\n",
        "np.random.seed(8942764)\n",
        "\n",
        "bert_cls = BertForTextClassification('bert-base-uncased', 2, freeze_bert=False)\n",
        "\n",
        "print(f'num_trainable_params={sum([p.numel() for p in bert_cls.parameters() if p.requires_grad])}\\n')\n",
        "\n",
        "bert_cls, bert_cls_logs = train(bert_cls, dataset['train'], dataset['validation'],\n",
        "                                num_epochs=10, batch_size=16, optimizer_cls='Adam',\n",
        "                                lr=2e-5, weight_decay=1e-4, device=device,\n",
        "                                collate_fn=tokenize)\n",
        "\n",
        "# this is where you run the test data (from huggingface) over the trained model and compute test loss and test accuracy\n",
        "print('\\n')\n",
        "print('Starting test run')\n",
        "test_loss, test_acc=evaluate(bert_cls,dataset['test'],batch_size=16, device=device, collate_fn=tokenize)\n",
        "print(f'Test Complete.\\t Test Loss: {test_loss:.3e}\\t Test Accuracy: {test_acc:.3f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}